---
layout: post
title:  "MongoDB"
date:   2020-05-02 11:18:12 -0700
categories: database
---

## MongoDB
- NoSQL 
* `mongodb+srv://<username>:<pwd>@<host>/<database>`. The hostname does not necessarily imply any cluster configuration. 

## Use MongoDB
__Using MongoDB Atlas__
1. Create a cluster
2. Load sample dataset
3. setup your IP and credentials and obtain the connection URI

__Install mongodb on Windows__
1. create the data folders to store your databases
2. setup alias shorts for mongo and mongod. 
    - open GitBash
    - Change dir to your home dir by `cd ~`
    - create bash_profile `touch .bach_profile`
    - put in the following in .bash_profile
    - 
    ```
    alias mongod="C:/Program\ Files/MongoDB/Server/4.2/bin/mongod.exe"
    alias mongo="C:/Program\ Files/MongoDB/Server/4.2/bin/mongo.exe"
    ```
    - verify the setup by 
        - close down current terminal
        - re-launch git bash
        - type `mongo --version`, you shall see mongoDB shell version, build env, etc

## BSON
`pip install bson`: for BSON (binary JSON) encoding and decoding

## Python Client
### Pymongo 
__Installation__
Note: need to install bson first, then install pymongo to avoid some errors

__Read__
```
client = pymongo.MongoClient(uri)
mflix = client.sample_mflix

# movies is our collection handle - it refers to the sample_mflix.movies collection
movies = mflix.movies
# find all movies with Salma Hayek
# then pretty print
cursor = movies.find( { "cast": "Salma Hayek" } )
from bson.json_util import dumps
print(dumps(cursor, indent=2))
```
* reading with `find_one()`: returns a record directly; reading with `find()`: return an iterable, need to iterating through cursors to read records. 
* use cursor to save results as Python iterables. Note that the cursor object expires after being called once. 
* we here use bson.json_util to dump cursor object and get it printed nicely
* you could also use `list(cursor)` to return a list of records contained in the cursor. 
* using "cast.actor" to project sub-field of "cast" field

__Cursor Methods and Aggregation equalvalents__
Aggregation: It is a MongoDB operation. Basically it is composed of a pipeline of operations (a list of stages of query operation dict) to query records from db. Pymongo also has aggregate methods by `<collection>.aggregate(<pipeline list>)`. 

Cursor methods are methods available in pymongo. The following cursor methods have MongoDB equals: 
* limit(n): number of outputs
* sort([("field name", "sorting order"), (...)])
* skip(n): skip the first n items. 

__Writes__
* insert_one: the return object contains "_id" of the inserted object, checked by `inserted_result.inserted_id`, and it tells whether the operation is acknowedged by the server, checked by `inserted_result.acknowledged` which returns a Boolean value. 
* upsert version in updates: Sometimes, we want to update a document, but we're not sure if it exists in the collection. We can use an "upsert" to update a document if it exists, and insert it if it does not exist.
This operation may do one of two things:
    * If the predicate matches a document, update the document to contain the correct data.
    * If the document doesn't exist, create the desired document.
    ```
    upsert_result = vg.update_one( { "title": "Fortnite" } , { "$set": fortnite_doc }, upsert=True )
    upsert_result.raw_result
    ```
    * this result object should have 'updatedExisting': True, because this operation updated an existing document. If it is insertion of a new record, 'updatedExisiting': False. 
    * besides, ".raw_result" include information such as number of updates, operationTime, ObjectId, clusterTime etc. 
    * "update_one({query}, {update operation}, upsert=True)". In this example, the query predicate here is { "title": "Fortnite" }. 
    * if have multiple items in the query or update operation, put them all in the curl bracket, e.g., use the user_email and comment_id to select the proper comment, then
    update the "text" and "date" of the selected comment. 
    `response = db.comments.update_one(
        { "email": user_email, "_id": ObjectId(comment_id)},
        { "$set": { "text": text , "date": date}}
    )`
* updates: update operation (update_one, update_many) return a UpdateResult. acknowledged, matched_count, modified_count, and upserted_id. modified_count and matched_count will be 0 in the case of an upsert.  

__Write concerns__
{w: n}: where n is the number of nodes get committed before the client receives acknowledgement. 
* {w: 1}: default writeConcern, it makes sure writes have been commited by at least 1 node. 
* {w: 'marjority'}: requests acknowledgement made from server to client if the majority of nodes in the replica set applied the write from the primary db. It takes longer than {w:1}, and it is more durable because it ensures vital writes are majority-committed. 
* {w: 0}: don't request an acknowledgement that any nodes applied the write (i.e., client could get acknowledgement before data is actually written to any db - primary or secondary dbs). It is fastest writeConcern level, and least durable. 

Set write concern
* for client-wise: `client = MongoClient(w="majority"), then check client.write_concern -> WriteConcern(w=majority)`
* for db: `from pymongo import WriteConcern, db = client.get_database("my_database",write_concern=WriteConcern(w="majority"))`
* for collection `from pymongo import WriteConcern coll2 = collection.with_options(write_concern=WriteConcern(w="majority"))
oid = coll2.insert({"a": 2})`

__Bulk writes__

Main idea: you have a sequence of write operations, each depending on preceding ones, if send them one by one will cause a lot of latencies, thus bulk them together, send the entire batch to MongoDB and get only one acknowledge back for the whole batch. Make multiple writes more efficient. 

* Ordered Bulk Write: 
    * default in Mongo
    * it executes write sequentially, will end execution after first write failure. 
* Unordered Bulk Write: 
    * has to specify with the flag "{ordered: false}"
    * executes writes in parallel
    * single failure won't prevent other writes from proceeding 

An example of ordered bulk write in pymongo: 
```
bulk_updates = [UpdateOne(
        {"_id": movie.get("doc_id")},
        {"$set": {"lastupdated": movie.get("lastupdated")}}
    ) for movie in movies_to_migrate]

    # here's where the bulk operation is sent to MongoDB
    bulk_results = mflix.movies.bulk_write(bulk_updates)
```
* bulk_updates: a sequence/list of pymongo operations
* use "bulk_write" to feed in bulk_updates. 



__Join__
* "let" allows us to declare variables in our pipeline, referring to document fields in our source collection

An example of pipeline that finds movies of certain id (pipeline stage 1), and aggregate comments from comments collection by joining its movie_id with id from movies collection, and returns comments are saved in an array as the "comments" field under each doc in movies collection. (lookup pipeline stage 1), all comments is ordered in the descending order in the array (lookup pipeline stage 2). 
```
pipeline = [
            {
                "$match": {
                    "_id": ObjectId(id)
                }
            }
        ]
        pipeline.extend([
            {
                '$lookup': {
                    'from': 'comments', 
                    'let': {'id': '$_id'}, 
                    'pipeline': [
                        {
                            '$match': {'$expr': {'$eq': ['$movie_id', '$$id']}}
                        }, 
                        {
                            '$sort': {'date': -1}
                        }
                    ], 
                    'as': 'comments'
                }
            }, 
            ])

        movie = db.movies.aggregate(pipeline).next()
```

__Aggregation: Group and Sort and Limit__
```
pipeline = [
    {
        '$group': {
            '_id': '$email', 
            'count': {'$sum': 1}
        }
    }, 
    {'$sort': {'count': -1}},
    {'$limit': 20}
    ]
```
* group "comments" collection by user email, select top 20 most commented users. return a list "_id: email, count: <counts>".

__Delete__
* The number of documents deleted can be viewed via "deleted_count" property on the DeleteResult object returned from the delete operation. 


__Read Concern__

The concern happens when the primary node writes the data, and not yet to pass to the secondary nodes. The primary suddenly goes down, and one of the secondary node becomes a primary and the primary becomes a secondary. Then the data rolls back to match the secondary db to the state of the primary db.

* default read is "local", meaning that it reads whatever copy of the data in the primary node regardless of whether or not the other nodes have replicated the data. 
* the read concern "majority: allows for more durable reads (meaning that slimly likely to be a rollback), it only returns data that has been replicated to a majority of nodes. e.g., the primary node is on 666, but two secondary nodes are still on 665, then reads will return 655 on a 3-node system as majority nodes are still at this state. 
* Integer values are not valid read concerns.
* in pymongo, `rc = pymongo.read_concern.ReadConcern(level='majority')
    comments = db.comments.with_options(read_concern=rc)`, rc level must be a ReadConcern object. 

__Connection Pooling__
* it allows for reuse of connections
* main benefits: 
    * New operations can be serviced with pre-existing connections and subsequent requests appear faster to the client. The overhead of creating a TCP connection often results in waiting time, but this is avoided by reusing a connection.
    * A large influx of operations can be handled more quickly with a pool of existing connections. Because the application has created a lot of available connections before they are needed, it has the bandwidth to service as many requests as connections. 
* by default, the connection pools are 100 connections. 
* connection pools are specific to a client, and the number of connections is declared when the client is initialized. 
* all connections in the pool are dropped after the client is terminated.
* In pymongo, `client = MongoClient("mongodb://localhost:27017/", maxPoolSize=50)`, set within MongoClient

__Robust Client Configuration__
* always use connection pooling
* For any writes with majority writes operation, always specify a "wtimeout". use majority `{w: "majority", wtimeout: 5000}`, timeout is in milli-sec. "wtimeout" is only relevant to write concern, not to read concern, connection pools etc. 
* always configure for and handle "serverSelectionTimeout" errors. by default, serverSelectionTimeout is 30 secs. 

Setting timeouts (e.g., serverSelectionTimeout, wtimeout etc) will make us aware of any software/hardware problems that haven't been recovered in an adquate amount of time. 

For example, wtimeout is specified in MongoClient as below `db = MongoClient(MFLIX_DB_URI,wTimeoutMS = 2500)[MFLIX_DB_NAME]`


__Error Handling__
* same strategy as in normal python scripts, use "try except" code block. 
* When handling errors, determine how fatal this error is. can we react to this error in a useful way? (e.g., analyze errors in "except" block.) if this is really fatal, we need to return it back to the user? 
* The exact exception that will be thrown in this cases: [pymongo](https://pymongo.readthedocs.io/en/stable/api/pymongo/errors.html), [BSON](https://pymongo.readthedocs.io/en/stable/api/bson/errors.html)

__Principle of Least Privilege__

Main idea: every program/user of the system should operate using the least amount of necessary to complete the job. 

Consider what kinds of users and what permission they will have: 
* application users: that log into the application itself
* database users: 
    * admin database users can create indexes, import data and so on. 
    * application database users only have privileges they require (i.e., apply operations against database). 


__Change Streams__
* report changes in the collection level
* accept pipelines to transform change events. 
* Change streams can stay open indefinitely.

```
try:
    with inventory.watch(full_document='updateLookup') as change_stream_cursor:
        for data_change in change_stream_cursor:
            print(data_change)
except pymongo.errors.PyMongoError:
    print('Change stream closed because of an error.')
```
* it watches "inventory" collection and 


## MongoDB Compass
Users can specify aggregation under "aggragation" tab, and export pipeline codes to language by clicking the "export" button. 

## Motor
`pip install motor`: asyc python driver for mongoDB
`client = motor.motor_asyncio.AsyncIOMotorClient(Databse_URL)`: create a new connection to a single MongoDB instance at host:port (database_url)

__Get database__
- `client.<database name>` or `client['database_name']`: a single instance of MongoDB can support multiple independent databases. 

## MongoDB terminology
- database: a collection of multiple tables
- a collection: one table
- document: one row in the table
- field: column
- table joins: MongoDB doesn't support
- "_id": MongoDB will use "_id" as primary key (unique index) to the document

## Mongod
Mongod is the mongodb daemon process, mainly used to start mongodb service. We could use one window to type in `mongod` to start mongoDB, and in another window, through `mongo` to link to database. 

## The mongo Shell
1. to connect to MongoDB instance running on your localhost default 27017 by typing `mongo`, connect to a non-default port `mongo --port 28015`, this will lead to a JS interative window, where you can type more commands in, need to end command with ";" because of JS. 
In that interative window: 
## Shundown running instance and restart
`db.adminCommand({shutdown: 1})

### listing databases 
`show databases`
### go to a particular databse 
`use <your_db_name>`, if this <your_db_name> is not present, then MongoDB server will create the database for you
### insert data
After switch to <you_db_name>, 
- `db.myCollection.insert({})`: insert documents as many as you want
- `db.myCollection.insertOne({})`: insert single document
- `db.myCollection.insertMany({})`: similar to `insert`, it can insert more than one document

### Query data
`db.myCollection.find({age: {$lt: 25}})`, or `db.myCollection.find().pretty()`: will display document in pretty-printed JSON format

`$lt`: special token, stand for less than

### Remove a document
`db.myCollection.remove({name: "john"})`: delete a collection

### Exit the shell
`quit()`, or Ctrl-C

## Replication
Main objective: replicate data and sync data across multiple server, improve the usability and security. 
Why replicate: - disaster recovery, allow you to recover data from hardware trouble or server break down. 

### Replication Main theory
Replication needs at least two nodes, one and only one must be "primary node", rest are secondary nodes. Common are one primary-one secondary, or one primary-multiple secondary. 

- primary node: receives all read and write operations from client app, it then records all changes to its datasets in its operation log (oplog)
- secondary node: secondaries replicate the primary's oplog and apply the operations to their datasets in an asyc process. 

All replicata set members contain a copy of oplog in "local.oplog.rs", allowing them to maintain the current state of the database. 

## ObjectId
0. default id for Mongodb. 
1. it is mostly monotonically increasing. Because it contains 4-bits of timestamp. Timestamp is in seconds, not milliseconds, so within the same second, the order of value is not guaranteed. 
2. Using ObjectId is efficient for indexing (means querying documents) and sorting. 

 